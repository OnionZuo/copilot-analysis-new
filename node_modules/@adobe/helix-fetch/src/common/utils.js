var require_utils = __commonJSMin((exports, module) => {
  "use strict";

  var {
      constants: {
        MAX_LENGTH: maxBufferLength
      }
    } = require("buffer"),
    {
      pipeline: pipeline,
      PassThrough: PassThrough
    } = require("stream"),
    {
      promisify: promisify
    } = require("util"),
    {
      createGunzip: createGunzip,
      createInflate: createInflate,
      createBrotliDecompress: createBrotliDecompress,
      constants: {
        Z_SYNC_FLUSH: Z_SYNC_FLUSH
      }
    } = require("zlib"),
    debug = b_()("helix-fetch:utils"),
    asyncPipeline = promisify(pipeline),
    canDecode = __name((statusCode, headers) => statusCode === 204 || statusCode === 304 || +headers["content-length"] == 0 ? !1 : /^\s*(?:(x-)?deflate|(x-)?gzip|br)\s*$/.test(headers["content-encoding"]), "canDecode"),
    decodeStream = __name((statusCode, headers, readableStream, onError) => {
      if (!canDecode(statusCode, headers)) return readableStream;
      let cb = __name(err => {
        err && (debug(`encountered error while decoding stream: ${err}`), onError(err));
      }, "cb");
      switch (headers["content-encoding"].trim()) {
        case "gzip":
        case "x-gzip":
          return pipeline(readableStream, createGunzip({
            flush: Z_SYNC_FLUSH,
            finishFlush: Z_SYNC_FLUSH
          }), cb);
        case "deflate":
        case "x-deflate":
          return pipeline(readableStream, createInflate(), cb);
        case "br":
          return pipeline(readableStream, createBrotliDecompress(), cb);
        default:
          return readableStream;
      }
    }, "decodeStream"),
    isPlainObject = __name(val => {
      if (!val || typeof val != "object" || Object.prototype.toString.call(val) !== "[object Object]") return !1;
      if (Object.getPrototypeOf(val) === null) return !0;
      let proto = val;
      for (; Object.getPrototypeOf(proto) !== null;) proto = Object.getPrototypeOf(proto);
      return Object.getPrototypeOf(val) === proto;
    }, "isPlainObject"),
    calcSize = __name((obj, processed) => {
      if (Buffer.isBuffer(obj)) return obj.length;
      switch (typeof obj) {
        case "string":
          return obj.length * 2;
        case "boolean":
          return 4;
        case "number":
          return 8;
        case "symbol":
          return Symbol.keyFor(obj) ? Symbol.keyFor(obj).length * 2 : (obj.toString().length - 8) * 2;
        case "object":
          return Array.isArray(obj) ? calcArraySize(obj, processed) : calcObjectSize(obj, processed);
        default:
          return 0;
      }
    }, "calcSize"),
    calcArraySize = __name((arr, processed) => (processed.add(arr), arr.map(entry => processed.has(entry) ? 0 : calcSize(entry, processed)).reduce((acc, curr) => acc + curr, 0)), "calcArraySize"),
    calcObjectSize = __name((obj, processed) => {
      if (obj == null) return 0;
      processed.add(obj);
      let bytes = 0,
        names = [];
      for (let key in obj) names.push(key);
      return names.push(...Object.getOwnPropertySymbols(obj)), names.forEach(nm => {
        if (bytes += calcSize(nm, processed), typeof obj[nm] == "object" && obj[nm] !== null) {
          if (processed.has(obj[nm])) return;
          processed.add(obj[nm]);
        }
        bytes += calcSize(obj[nm], processed);
      }), bytes;
    }, "calcObjectSize"),
    sizeof = __name(obj => calcSize(obj, new WeakSet()), "sizeof"),
    streamToBuffer = __name(async stream => {
      let passThroughStream = new PassThrough(),
        length = 0,
        chunks = [];
      return passThroughStream.on("data", chunk => {
        if (length + chunk.length > maxBufferLength) throw new Error("Buffer.constants.MAX_SIZE exceeded");
        chunks.push(chunk), length += chunk.length;
      }), await asyncPipeline(stream, passThroughStream), Buffer.concat(chunks, length);
    }, "streamToBuffer");
  module.exports = {
    decodeStream: decodeStream,
    isPlainObject: isPlainObject,
    sizeof: sizeof,
    streamToBuffer: streamToBuffer
  };
});