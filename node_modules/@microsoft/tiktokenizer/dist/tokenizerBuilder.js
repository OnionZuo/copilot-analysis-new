var require_tokenizerBuilder = __commonJSMin(exports => {
  "use strict";

  Object.defineProperty(exports, "__esModule", {
    value: !0
  });
  exports.createTokenizer = exports.createByEncoderName = exports.createByModelName = exports.getRegexByModel = exports.getRegexByEncoder = exports.getSpecialTokensByModel = exports.getSpecialTokensByEncoder = exports.MODEL_TO_ENCODING = void 0;
  var fs = require("fs"),
    path = require("path"),
    tikTokenizer_1 = hY(),
    MODEL_PREFIX_TO_ENCODING = new Map([["gpt-4o-", "o200k_base"], ["gpt-4-", "cl100k_base"], ["gpt-3.5-turbo-", "cl100k_base"], ["gpt-35-turbo-", "cl100k_base"]]);
  exports.MODEL_TO_ENCODING = new Map([["gpt-4o", "o200k_base"], ["gpt-4", "cl100k_base"], ["gpt-3.5-turbo", "cl100k_base"], ["text-davinci-003", "p50k_base"], ["text-davinci-002", "p50k_base"], ["text-davinci-001", "r50k_base"], ["text-curie-001", "r50k_base"], ["text-babbage-001", "r50k_base"], ["text-ada-001", "r50k_base"], ["davinci", "r50k_base"], ["curie", "r50k_base"], ["babbage", "r50k_base"], ["ada", "r50k_base"], ["code-davinci-002", "p50k_base"], ["code-davinci-001", "p50k_base"], ["code-cushman-002", "p50k_base"], ["code-cushman-001", "p50k_base"], ["davinci-codex", "p50k_base"], ["cushman-codex", "p50k_base"], ["text-davinci-edit-001", "p50k_edit"], ["code-davinci-edit-001", "p50k_edit"], ["text-embedding-ada-002", "cl100k_base"], ["text-similarity-davinci-001", "r50k_base"], ["text-similarity-curie-001", "r50k_base"], ["text-similarity-babbage-001", "r50k_base"], ["text-similarity-ada-001", "r50k_base"], ["text-search-davinci-doc-001", "r50k_base"], ["text-search-curie-doc-001", "r50k_base"], ["text-search-babbage-doc-001", "r50k_base"], ["text-search-ada-doc-001", "r50k_base"], ["code-search-babbage-code-001", "r50k_base"], ["code-search-ada-code-001", "r50k_base"], ["gpt2", "gpt2"]]);
  var ENDOFTEXT = "<|endoftext|>",
    FIM_PREFIX = "<|fim_prefix|>",
    FIM_MIDDLE = "<|fim_middle|>",
    FIM_SUFFIX = "<|fim_suffix|>",
    ENDOFPROMPT = "<|endofprompt|>",
    REGEX_PATTERN_1 = "'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+",
    REGEX_PATTERN_2 = "(?:'s|'S|'t|'T|'re|'RE|'Re|'eR|'ve|'VE|'vE|'Ve|'m|'M|'ll|'lL|'Ll|'LL|'d|'D)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+",
    patterns = [`[^\r
\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?:'s|'S|'t|'T|'re|'RE|'Re|'eR|'ve|'VE|'vE|'Ve|'m|'M|'ll|'lL|'Ll|'LL|'d|'D)?`, `[^\r
\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?:'s|'S|'t|'T|'re|'RE|'Re|'eR|'ve|'VE|'vE|'Ve|'m|'M|'ll|'lL|'Ll|'LL|'d|'D)?`, "\\p{N}{1,3}", " ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*", "\\s*[\\r\\n]+", "\\s+(?!\\S)", "\\s+"],
    REGEX_PATTERN_3 = patterns.join("|");
  function getEncoderFromModelName(modelName) {
    let encoder = "";
    if (exports.MODEL_TO_ENCODING.has(modelName)) encoder = exports.MODEL_TO_ENCODING.get(modelName);else for (let [prefix, encoding] of MODEL_PREFIX_TO_ENCODING) if (modelName.startsWith(prefix)) {
      encoder = encoding;
      break;
    }
    return encoder;
  }
  __name(getEncoderFromModelName, "getEncoderFromModelName");
  async function fetchAndSaveFile(mergeableRanksFileUrl, filePath) {
    let response = await fetch(mergeableRanksFileUrl);
    if (!response.ok) throw new Error(`Failed to fetch file from ${mergeableRanksFileUrl}. Status code: ${response.status}`);
    let text = await response.text();
    fs.writeFileSync(filePath, text);
  }
  __name(fetchAndSaveFile, "fetchAndSaveFile");
  function getSpecialTokensByEncoder(encoder) {
    let specialTokens = new Map([[ENDOFTEXT, 50256]]);
    switch (encoder) {
      case "o200k_base":
        specialTokens = new Map([[ENDOFTEXT, 199999], [ENDOFPROMPT, 200018]]);
        break;
      case "cl100k_base":
        specialTokens = new Map([[ENDOFTEXT, 100257], [FIM_PREFIX, 100258], [FIM_MIDDLE, 100259], [FIM_SUFFIX, 100260], [ENDOFPROMPT, 100276]]);
        break;
      case "p50k_edit":
        specialTokens = new Map([[ENDOFTEXT, 50256], [FIM_PREFIX, 50281], [FIM_MIDDLE, 50282], [FIM_SUFFIX, 50283]]);
        break;
      default:
        break;
    }
    return specialTokens;
  }
  __name(getSpecialTokensByEncoder, "getSpecialTokensByEncoder");
  exports.getSpecialTokensByEncoder = getSpecialTokensByEncoder;
  function getSpecialTokensByModel(modelName) {
    let encoderName = getEncoderFromModelName(modelName);
    return getSpecialTokensByEncoder(encoderName);
  }
  __name(getSpecialTokensByModel, "getSpecialTokensByModel");
  exports.getSpecialTokensByModel = getSpecialTokensByModel;
  function getRegexByEncoder(encoder) {
    switch (encoder) {
      case "o200k_base":
        return REGEX_PATTERN_3;
      case "cl100k_base":
        return REGEX_PATTERN_2;
      default:
        break;
    }
    return REGEX_PATTERN_1;
  }
  __name(getRegexByEncoder, "getRegexByEncoder");
  exports.getRegexByEncoder = getRegexByEncoder;
  function getRegexByModel(modelName) {
    let encoderName = getEncoderFromModelName(modelName);
    return getRegexByEncoder(encoderName);
  }
  __name(getRegexByModel, "getRegexByModel");
  exports.getRegexByModel = getRegexByModel;
  async function createByModelName(modelName, extraSpecialTokens = null) {
    return createByEncoderName(getEncoderFromModelName(modelName), extraSpecialTokens);
  }
  __name(createByModelName, "createByModelName");
  exports.createByModelName = createByModelName;
  async function createByEncoderName(encoderName, extraSpecialTokens = null) {
    let regexPattern,
      mergeableRanksFileUrl,
      specialTokens = getSpecialTokensByEncoder(encoderName);
    switch (encoderName) {
      case "o200k_base":
        regexPattern = REGEX_PATTERN_3, mergeableRanksFileUrl = "https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken";
        break;
      case "cl100k_base":
        regexPattern = REGEX_PATTERN_2, mergeableRanksFileUrl = "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken";
        break;
      case "p50k_base":
        regexPattern = REGEX_PATTERN_1, mergeableRanksFileUrl = "https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken";
        break;
      case "p50k_edit":
        regexPattern = REGEX_PATTERN_1, mergeableRanksFileUrl = "https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken";
        break;
      case "r50k_base":
        regexPattern = REGEX_PATTERN_1, mergeableRanksFileUrl = "https://openaipublic.blob.core.windows.net/encodings/r50k_base.tiktoken";
        break;
      case "gpt2":
        regexPattern = REGEX_PATTERN_1, mergeableRanksFileUrl = "https://raw.githubusercontent.com/microsoft/Tokenizer/main/model/gpt2.tiktoken";
        break;
      default:
        throw new Error(`Doesn't support this encoder [${encoderName}]`);
    }
    extraSpecialTokens !== null && (specialTokens = new Map([...specialTokens, ...extraSpecialTokens]));
    let fileName = path.basename(mergeableRanksFileUrl),
      dirPath = path.resolve(__dirname, "..", "model");
    fs.existsSync(dirPath) || fs.mkdirSync(dirPath, {
      recursive: !0
    });
    let filePath = path.resolve(dirPath, fileName);
    return fs.existsSync(filePath) || (console.log(`Downloading file from ${mergeableRanksFileUrl}`), await fetchAndSaveFile(mergeableRanksFileUrl, filePath), console.log(`Saved file to ${filePath}`)), createTokenizer(filePath, specialTokens, regexPattern);
  }
  __name(createByEncoderName, "createByEncoderName");
  exports.createByEncoderName = createByEncoderName;
  function createTokenizer(tikTokenBpeFileOrDict, specialTokensEncoder, regexPattern, cacheSize = 8192) {
    return new tikTokenizer_1.TikTokenizer(tikTokenBpeFileOrDict, specialTokensEncoder, regexPattern, cacheSize);
  }
  __name(createTokenizer, "createTokenizer");
  exports.createTokenizer = createTokenizer;
});