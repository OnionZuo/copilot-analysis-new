var require_tikTokenizer = __commonJSMin(exports => {
  "use strict";

  Object.defineProperty(exports, "__esModule", {
    value: !0
  });
  exports.TikTokenizer = void 0;
  var fs = require("fs"),
    util_1 = require("util"),
    bytePairEncode_1 = jve(),
    textEncoder_1 = Uve(),
    lru_1 = Ove();
  function loadTikTokenBpe(tikTokenBpeFile) {
    let bpeDict = new Map();
    try {
      let fileContent = fs.readFileSync(tikTokenBpeFile, "utf-8");
      return processBpeRanks(fileContent), bpeDict;
    } catch (ex) {
      throw new Error(`Failed to load from BPE encoder file stream: ${ex}`);
    }
    function processBpeRanks(fileContent) {
      for (let line of fileContent.split(/[\r\n]+/)) {
        if (line.trim() === "") continue;
        let tokens = line.split(" ");
        if (tokens.length !== 2) throw new Error("Invalid format in the BPE encoder file stream");
        let tokenBytes = new Uint8Array(Buffer.from(tokens[0], "base64")),
          rank = parseInt(tokens[1]);
        if (!isNaN(rank)) bpeDict.set(tokenBytes, rank);else throw new Error(`Can't parse ${tokens[1]} to integer`);
      }
    }
    __name(processBpeRanks, "processBpeRanks");
  }
  __name(loadTikTokenBpe, "loadTikTokenBpe");
  function escapeRegExp(regex) {
    return regex.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
  }
  __name(escapeRegExp, "escapeRegExp");
  var _TikTokenizer = class _TikTokenizer {
    constructor(tikTokenBpeFileOrDict, specialTokensEncoder, regexPattern, cacheSize = 8192) {
      this.textEncoder = (0, textEncoder_1.makeTextEncoder)(), this.textDecoder = new util_1.TextDecoder("utf-8"), this.cache = new lru_1.LRUCache(cacheSize);
      let bpeDict = typeof tikTokenBpeFileOrDict == "string" ? loadTikTokenBpe(tikTokenBpeFileOrDict) : tikTokenBpeFileOrDict;
      this.init(bpeDict, specialTokensEncoder, regexPattern);
    }
    init(bpeDict, specialTokensEncoder, regexPattern) {
      this.encoder = new bytePairEncode_1.BinaryMap();
      for (let [key, value] of bpeDict) this.encoder.set(key, value);
      this.regex = new RegExp(regexPattern, "gu"), this.specialTokensRegex = new RegExp(Array.from(specialTokensEncoder.keys()).map(s => escapeRegExp(s)).join("|")), this.specialTokensEncoder = specialTokensEncoder, this.decoder = new Map();
      for (let [key, value] of bpeDict) this.decoder.set(value, key);
      if (bpeDict.size !== this.decoder.size) throw new Error("Encoder and decoder sizes do not match");
      this.specialTokensDecoder = new Map();
      for (let [key, value] of specialTokensEncoder) this.specialTokensDecoder.set(value, key);
    }
    findNextSpecialToken(text, start, allowedSpecial) {
      let startFind = start,
        nextSpecial = null;
      if (allowedSpecial && this.specialTokensRegex) for (; nextSpecial = text.slice(startFind).match(this.specialTokensRegex), !(!nextSpecial || allowedSpecial && allowedSpecial.includes(nextSpecial[0]));) startFind += nextSpecial.index + 1;
      let end = nextSpecial ? startFind + nextSpecial.index : text.length;
      return [nextSpecial, end];
    }
    encode(text, allowedSpecial) {
      let tokenIds = [],
        start = 0;
      for (;;) {
        let nextSpecial, end;
        if ([nextSpecial, end] = this.findNextSpecialToken(text, start, allowedSpecial), end > start && this.encodeByIndex(text, tokenIds, start, end), nextSpecial) {
          if (start = start + this.encodeSpecialToken(tokenIds, nextSpecial), start >= text.length) break;
        } else break;
      }
      return tokenIds;
    }
    encodeSpecialToken(tokenIds, nextSpecial) {
      var _a;
      let token = (_a = this.specialTokensEncoder) == null ? void 0 : _a.get(nextSpecial[0]);
      return tokenIds.push(token), nextSpecial.index + nextSpecial[0].length;
    }
    encodeByIndex(text, tokenIds, start, end) {
      let match,
        substring = text.substring(start, end);
      for (this.regex.lastIndex = 0; match = this.regex.exec(substring);) {
        let cached = this.cache.get(match[0]);
        if (cached) for (let b of cached) tokenIds.push(b);else {
          let bytes = this.textEncoder.encode(match[0]),
            token = this.encoder.get(bytes, 0, this.textEncoder.length);
          if (token !== void 0) tokenIds.push(token), this.cache.set(match[0], [token]);else {
            let encodedTokens = (0, bytePairEncode_1.bytePairEncode)(bytes, this.encoder, this.textEncoder.length);
            for (let b of encodedTokens) tokenIds.push(b);
            this.cache.set(match[0], encodedTokens);
          }
        }
      }
    }
    encodeTrimSuffixByIndex(text, tokenIds, start, end, maxTokenCount, tokenCount, encodeLength) {
      let match,
        substring = text.substring(start, end);
      for (this.regex.lastIndex = 0; match = this.regex.exec(substring);) {
        let piece = match[0],
          cachedTokens = this.cache.get(piece);
        if (cachedTokens) {
          if (tokenCount + cachedTokens.length <= maxTokenCount) tokenCount += cachedTokens.length, encodeLength += piece.length, tokenIds.push(...cachedTokens);else {
            let remainingTokens = maxTokenCount - tokenCount;
            tokenCount += remainingTokens, encodeLength += piece.length, tokenIds.push(...cachedTokens.slice(0, remainingTokens));
            break;
          }
        } else {
          let bytes = this.textEncoder.encode(piece),
            token = this.encoder.get(bytes, 0, bytes.length);
          if (token !== void 0) {
            if (this.cache.set(piece, [token]), tokenCount + 1 <= maxTokenCount) tokenCount++, encodeLength += piece.length, tokenIds.push(token);else break;
          } else {
            let encodedTokens = (0, bytePairEncode_1.bytePairEncode)(bytes, this.encoder, this.textEncoder.length);
            if (this.cache.set(piece, encodedTokens), tokenCount + encodedTokens.length <= maxTokenCount) {
              tokenCount += encodedTokens.length, encodeLength += piece.length;
              for (let b of encodedTokens) tokenIds.push(b);
            } else {
              let remainingTokens = maxTokenCount - tokenCount;
              tokenCount += remainingTokens, encodeLength += piece.length;
              for (let i = 0; i < remainingTokens; i++) tokenIds.push(encodedTokens[i]);
              break;
            }
          }
        }
        if (tokenCount >= maxTokenCount) break;
      }
      return {
        tokenCount: tokenCount,
        encodeLength: encodeLength
      };
    }
    encodeTrimSuffix(text, maxTokenCount, allowedSpecial) {
      let tokenIds = [],
        start = 0,
        tokenCount = 0,
        encodeLength = 0;
      for (;;) {
        let nextSpecial, end;
        if ([nextSpecial, end] = this.findNextSpecialToken(text, start, allowedSpecial), end > start) {
          let {
            tokenCount: newTokenCount,
            encodeLength: newEncodeLength
          } = this.encodeTrimSuffixByIndex(text, tokenIds, start, end, maxTokenCount, tokenCount, encodeLength);
          if (tokenCount = newTokenCount, encodeLength = newEncodeLength, tokenCount >= maxTokenCount) break;
        }
        if (nextSpecial !== null) {
          if (tokenCount++, tokenCount <= maxTokenCount && (start = start + this.encodeSpecialToken(tokenIds, nextSpecial), encodeLength += nextSpecial[0].length, start >= text.length) || tokenCount >= maxTokenCount) break;
        } else break;
      }
      let encodedText = encodeLength === text.length ? text : text.slice(0, encodeLength);
      return {
        tokenIds: tokenIds,
        text: encodedText
      };
    }
    encodeTrimPrefix(text, maxTokenCount, allowedSpecial) {
      let tokenIds = [],
        start = 0,
        tokenCount = 0,
        encodeLength = 0,
        tokenCountMap = new Map();
      for (tokenCountMap.set(tokenCount, encodeLength);;) {
        let nextSpecial, end;
        if ([nextSpecial, end] = this.findNextSpecialToken(text, start, allowedSpecial), end > start) {
          let match,
            substring = text.substring(start, end);
          for (this.regex.lastIndex = 0; match = this.regex.exec(substring);) {
            let piece = match[0],
              cachedTokens = this.cache.get(piece);
            if (cachedTokens) tokenCount += cachedTokens.length, encodeLength += piece.length, tokenIds.push(...cachedTokens), tokenCountMap.set(tokenCount, encodeLength);else {
              let bytes = this.textEncoder.encode(piece),
                token = this.encoder.get(bytes);
              if (token !== void 0) this.cache.set(piece, [token]), tokenCount++, encodeLength += piece.length, tokenIds.push(token), tokenCountMap.set(tokenCount, encodeLength);else {
                let encodedTokens = (0, bytePairEncode_1.bytePairEncode)(bytes, this.encoder, this.textEncoder.length);
                this.cache.set(piece, encodedTokens), tokenCount += encodedTokens.length, encodeLength += piece.length;
                for (let b of encodedTokens) tokenIds.push(b);
                tokenCountMap.set(tokenCount, encodeLength);
              }
            }
          }
        }
        if (nextSpecial !== null) {
          if (start = start + this.encodeSpecialToken(tokenIds, nextSpecial), tokenCount++, encodeLength += nextSpecial[0].length, tokenCountMap.set(tokenCount, encodeLength), start >= text.length) break;
        } else break;
      }
      if (tokenCount <= maxTokenCount) return {
        tokenIds: tokenIds,
        text: text
      };
      let prefixTokenCount = tokenCount - maxTokenCount,
        actualPrefixTokenCount = 0,
        actualPrefixStrLength = 0;
      for (let [key, value] of tokenCountMap) if (key >= prefixTokenCount) {
        actualPrefixTokenCount = key, actualPrefixStrLength = value;
        break;
      }
      if (actualPrefixTokenCount > maxTokenCount) {
        let encodedTokens = this.encode(text, allowedSpecial),
          slicedTokens = encodedTokens.slice(encodedTokens.length - maxTokenCount);
        return {
          tokenIds: slicedTokens,
          text: this.decode(slicedTokens)
        };
      }
      return {
        tokenIds: tokenIds.slice(actualPrefixTokenCount),
        text: text.slice(actualPrefixStrLength)
      };
    }
    decode(tokens) {
      var _a, _b;
      let decoded = [];
      for (let token of tokens) {
        let tokenBytes = [],
          value = (_a = this.decoder) == null ? void 0 : _a.get(token);
        if (value !== void 0) tokenBytes = Array.from(value);else {
          let specialTokenValue = (_b = this.specialTokensDecoder) == null ? void 0 : _b.get(token);
          if (specialTokenValue !== void 0) {
            let bytes = this.textEncoder.encode(specialTokenValue);
            tokenBytes = Array.from(bytes.subarray(0, this.textEncoder.length));
          }
        }
        decoded.push(...tokenBytes);
      }
      return this.textDecoder.decode(new Uint8Array(decoded));
    }
  };
  __name(_TikTokenizer, "TikTokenizer");
  var TikTokenizer = _TikTokenizer;
  exports.TikTokenizer = TikTokenizer;
});